<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Opacus · Train PyTorch models with Differential Privacy</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="Train PyTorch models with Differential Privacy"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Opacus · Train PyTorch models with Differential Privacy"/><meta property="og:type" content="website"/><meta property="og:url" content="https://opacus.ai/"/><meta property="og:description" content="Train PyTorch models with Differential Privacy"/><meta property="og:image" content="https://opacus.ai/img/opacus_logo.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://opacus.ai/img/opacus_logo.svg"/><link rel="shortcut icon" href="/img/opacus_favicon.svg"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><script>
              (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
              (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
              m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
              })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

              ga('create', 'UA-117752657-3', 'auto');
              ga('send', 'pageview');
            </script><link rel="stylesheet" href="/css/code_block_buttons.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="/js/code_block_buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/js/mathjax.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML"></script><script src="https://unpkg.com/vanilla-back-to-top@7.1.14/dist/vanilla-back-to-top.min.js"></script><script>
        document.addEventListener('DOMContentLoaded', function() {
          addBackToTop(
            {"zIndex":100}
          )
        });
        </script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/opacus_logo.svg" alt="Opacus"/></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/docs/introduction" target="_self">Introduction</a></li><li class=""><a href="/docs/faq" target="_self">FAQ</a></li><li class=""><a href="/tutorials/" target="_self">Tutorials</a></li><li class=""><a href="/api/" target="_self">API Reference</a></li><li class=""><a href="https://github.com/pytorch/opacus" target="_self">GitHub</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search" title="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div>
<script type="text/javascript" id="documentation_options" data-url_root="./"
  src="/js/documentation_options.js"></script>
<script type="text/javascript" src="/js/jquery.js"></script>
<script type="text/javascript" src="/js/underscore.js"></script>
<script type="text/javascript" src="/js/doctools.js"></script>
<script type="text/javascript" src="/js/language_data.js"></script>
<script type="text/javascript" src="/js/searchtools.js"></script>
<div class="sphinx"><div class="document">
<div class="documentwrapper">
<div class="bodywrapper">
<div class="body" role="main">
<section id="module-opacus.layers.dp_lstm">
<span id="dplstm"></span><h1>DPLSTM<a class="headerlink" href="#module-opacus.layers.dp_lstm" title="Permalink to this headline">¶</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="opacus.layers.dp_lstm.BidirectionalDPLSTMLayer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">opacus.layers.dp_lstm.</span></span><span class="sig-name descname"><span class="pre">BidirectionalDPLSTMLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/opacus/layers/dp_lstm.html#BidirectionalDPLSTMLayer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#opacus.layers.dp_lstm.BidirectionalDPLSTMLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements <em>one</em> layer of Bidirectional LSTM in a way amenable to differential privacy.
We don’t expect you to use this directly: use DPLSTM instead :)</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="py method">
<dt class="sig sig-object py" id="opacus.layers.dp_lstm.BidirectionalDPLSTMLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_init</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_sizes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/opacus/layers/dp_lstm.html#BidirectionalDPLSTMLayer.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#opacus.layers.dp_lstm.BidirectionalDPLSTMLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the forward pass of the DPLSTM when a sequence is input.</p>
<dl class="simple">
<dt>Dimensions as follows:</dt><dd><ul class="simple">
<li><p>B: Batch size</p></li>
<li><p>T: Sequence length</p></li>
<li><p>D: LSTM input hidden size (eg from a word embedding)</p></li>
<li><p>H: LSTM output hidden size</p></li>
<li><p>P: number of directions (2 if bidirectional, else 1)</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>) – Input sequence to the DPLSTM of shape <code class="docutils literal notranslate"><span class="pre">[T,</span> <span class="pre">B,</span> <span class="pre">D]</span></code></p></li>
<li><p><strong>state_init</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.9)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>]) – <dl class="simple">
<dt>Initial state of the LSTM as a tuple <code class="docutils literal notranslate"><span class="pre">(h_0,</span> <span class="pre">c_0)</span></code>, where</dt><dd><p><code class="docutils literal notranslate"><span class="pre">h_0</span></code> of shape <code class="docutils literal notranslate"><span class="pre">[P,</span> <span class="pre">B,</span> <span class="pre">H]</span></code> contains the initial hidden state, and
<code class="docutils literal notranslate"><span class="pre">c_0</span></code> of shape <code class="docutils literal notranslate"><span class="pre">[P,</span> <span class="pre">B,</span> <span class="pre">H]</span></code>  contains the initial cell state. This
argument can be (and defaults to) None, in which case zero tensors
will be used.</p>
</dd>
<dt>Returns:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">output,</span> <span class="pre">(h_n,</span> <span class="pre">c_n)</span></code> where, <code class="docutils literal notranslate"><span class="pre">output</span></code> is of shape <code class="docutils literal notranslate"><span class="pre">[T,</span> <span class="pre">B,</span> <span class="pre">H</span> <span class="pre">*</span> <span class="pre">P]</span></code> and is a
tensor containing the output features (<code class="docutils literal notranslate"><span class="pre">h_t</span></code>) from the last layer of the
DPLSTM for each timestep <code class="docutils literal notranslate"><span class="pre">t</span></code>. <code class="docutils literal notranslate"><span class="pre">h_n</span></code> is of shape <code class="docutils literal notranslate"><span class="pre">[P,</span> <span class="pre">B,</span> <span class="pre">H]</span></code> and contains
the hidden state for <code class="docutils literal notranslate"><span class="pre">t</span> <span class="pre">=</span> <span class="pre">T</span></code>. <code class="docutils literal notranslate"><span class="pre">c_n</span></code> is of shape <code class="docutils literal notranslate"><span class="pre">[P,</span> <span class="pre">B,</span> <span class="pre">H]</span></code> and contains
the cell state for <code class="docutils literal notranslate"><span class="pre">t</span> <span class="pre">=</span> <span class="pre">T</span></code>.</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.9)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.9)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>]]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="opacus.layers.dp_lstm.BidirectionalDPLSTMLayer.set_max_batch_length">
<span class="sig-name descname"><span class="pre">set_max_batch_length</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_batch_length</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/opacus/layers/dp_lstm.html#BidirectionalDPLSTMLayer.set_max_batch_length"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#opacus.layers.dp_lstm.BidirectionalDPLSTMLayer.set_max_batch_length" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets max batch length</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.9)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="opacus.layers.dp_lstm.DPLSTM">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">opacus.layers.dp_lstm.</span></span><span class="sig-name descname"><span class="pre">DPLSTM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_first</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bidirectional</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/opacus/layers/dp_lstm.html#DPLSTM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#opacus.layers.dp_lstm.DPLSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>DP-friendly drop-in replacement of the <code class="docutils literal notranslate"><span class="pre">torch.nn.LSTM</span></code> module.</p>
<p>Its state_dict matches that of nn.LSTM exactly, so that after training it can be exported
and loaded by an nn.LSTM for inference.</p>
<p>Refer to nn.LSTM’s documentation for all parameters and inputs.</p>
<p>Initializes internal state. Subclass this instead of <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> whenever you need
to rename your model’s state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>rename_map</strong> – mapping from old name -&gt; new name for each parameter you want renamed.
Note that this must be a 1:1 mapping!</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="opacus.layers.dp_lstm.DPLSTM.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/opacus/layers/dp_lstm.html#DPLSTM.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#opacus.layers.dp_lstm.DPLSTM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the forward pass of the DPLSTM when a sequence is input.</p>
<dl class="simple">
<dt>Dimensions as follows:</dt><dd><ul class="simple">
<li><p>B: Batch size</p></li>
<li><p>T: Sequence length</p></li>
<li><p>D: LSTM input hidden size (eg from a word embedding)</p></li>
<li><p>H: LSTM output hidden size</p></li>
<li><p>L: number of layers in the LSTM</p></li>
<li><p>P: number of directions (2 if bidirectional, else 1)</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.9)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.PackedSequence.html#torch.nn.utils.rnn.PackedSequence" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">PackedSequence</span></code></a>]) – Input sequence to the DPLSTM of shape <code class="docutils literal notranslate"><span class="pre">[T,</span> <span class="pre">B,</span> <span class="pre">D]</span></code>. Or it can be a PackedSequence.</p></li>
<li><p><strong>state_init</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.9)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.9)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>]]) – <dl>
<dt>Initial state of the LSTM as a tuple <code class="docutils literal notranslate"><span class="pre">(h_0,</span> <span class="pre">c_0)</span></code>, where:</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">h_0</span></code> of shape <code class="docutils literal notranslate"><span class="pre">[L*P,</span> <span class="pre">B,</span> <span class="pre">H]</span></code> contains the initial hidden state</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">c_0</span></code> of shape <code class="docutils literal notranslate"><span class="pre">[L*P,</span> <span class="pre">B,</span> <span class="pre">H]</span></code> contains the initial cell state</p></li>
</ul>
<p>This argument can be (and defaults to) None, in which case zero tensors will be used.</p>
</dd>
<dt>Returns:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">output,</span> <span class="pre">(h_n,</span> <span class="pre">c_n)</span></code> where, <code class="docutils literal notranslate"><span class="pre">output</span></code> is of shape <code class="docutils literal notranslate"><span class="pre">[T,</span> <span class="pre">B,</span> <span class="pre">H</span> <span class="pre">*</span> <span class="pre">P]</span></code> and is a
tensor containing the output features (<code class="docutils literal notranslate"><span class="pre">h_t</span></code>) from the last layer of the DPLSTM
for each timestep <code class="docutils literal notranslate"><span class="pre">t</span></code>. <code class="docutils literal notranslate"><span class="pre">h_n</span></code> is of shape <code class="docutils literal notranslate"><span class="pre">[L</span> <span class="pre">*</span> <span class="pre">P,</span> <span class="pre">B,</span> <span class="pre">H]</span></code> and contains the
hidden state for <code class="docutils literal notranslate"><span class="pre">t</span> <span class="pre">=</span> <span class="pre">T</span></code>. <code class="docutils literal notranslate"><span class="pre">c_n</span></code> is of shape <code class="docutils literal notranslate"><span class="pre">[L</span> <span class="pre">*</span> <span class="pre">P,</span> <span class="pre">B,</span> <span class="pre">H]</span></code> and contains
the cell state for <code class="docutils literal notranslate"><span class="pre">t</span> <span class="pre">=</span> <span class="pre">T</span></code>.</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.9)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.9)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>]]</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="opacus.layers.dp_lstm.DPLSTMCell">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">opacus.layers.dp_lstm.</span></span><span class="sig-name descname"><span class="pre">DPLSTMCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/opacus/layers/dp_lstm.html#DPLSTMCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#opacus.layers.dp_lstm.DPLSTMCell" title="Permalink to this definition">¶</a></dt>
<dd><p>Internal-only class. Implements <em>one</em> step of LSTM so that a LSTM layer can be seen as repeated
applications of this class.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="py method">
<dt class="sig sig-object py" id="opacus.layers.dp_lstm.DPLSTMCell.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h_prev</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_prev</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size_t</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/opacus/layers/dp_lstm.html#DPLSTMCell.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#opacus.layers.dp_lstm.DPLSTMCell.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.9)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>]</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="opacus.layers.dp_lstm.DPLSTMCell.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/opacus/layers/dp_lstm.html#DPLSTMCell.reset_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#opacus.layers.dp_lstm.DPLSTMCell.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Resets parameters by initializing them from an uniform distribution.</p>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="opacus.layers.dp_lstm.DPLSTMCell.set_max_batch_length">
<span class="sig-name descname"><span class="pre">set_max_batch_length</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_batch_length</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/opacus/layers/dp_lstm.html#DPLSTMCell.set_max_batch_length"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#opacus.layers.dp_lstm.DPLSTMCell.set_max_batch_length" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets max batch length</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.9)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="opacus.layers.dp_lstm.DPLSTMLayer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">opacus.layers.dp_lstm.</span></span><span class="sig-name descname"><span class="pre">DPLSTMLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reverse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/opacus/layers/dp_lstm.html#DPLSTMLayer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#opacus.layers.dp_lstm.DPLSTMLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements <em>one</em> layer of LSTM in a way amenable to differential privacy.
We don’t expect you to use this directly: use DPLSTM instead :)</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="py method">
<dt class="sig sig-object py" id="opacus.layers.dp_lstm.DPLSTMLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_init</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_sizes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/opacus/layers/dp_lstm.html#DPLSTMLayer.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#opacus.layers.dp_lstm.DPLSTMLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the forward pass of the DPLSTMLayer when a sequence is given in input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.9)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.9)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code></a>]) – Input sequence to the DPLSTMCell of shape <code class="docutils literal notranslate"><span class="pre">[T,</span> <span class="pre">B,</span> <span class="pre">D]</span></code>.</p></li>
<li><p><strong>state_init</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.9)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>]) – Initial state of the LSTMCell as a tuple <code class="docutils literal notranslate"><span class="pre">(h_0,</span> <span class="pre">c_0)</span></code>
where <code class="docutils literal notranslate"><span class="pre">h_0</span></code> is the initial hidden state and <code class="docutils literal notranslate"><span class="pre">c_0</span></code> is the
initial cell state of the DPLSTMCell</p></li>
<li><p><strong>batch_sizes</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.9)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>]) – Contains the batch sizes as stored in PackedSequence</p></li>
</ul>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.9)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.9)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code></a>[<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>, <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.8.0a0+56b43f4)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a>]]</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">output,</span> <span class="pre">(h_n,</span> <span class="pre">c_n)</span></code> where, <code class="docutils literal notranslate"><span class="pre">output</span></code> is of shape <code class="docutils literal notranslate"><span class="pre">[T,</span> <span class="pre">B,</span> <span class="pre">H]</span></code> and is a
tensor containing the output features (<code class="docutils literal notranslate"><span class="pre">h_t</span></code>) from the last layer of the
DPLSTMCell for each timestep <code class="docutils literal notranslate"><span class="pre">t</span></code>. <code class="docutils literal notranslate"><span class="pre">h_n</span></code> is of shape <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">H]</span></code> and is a
tensor containing the hidden state for <code class="docutils literal notranslate"><span class="pre">t</span> <span class="pre">=</span> <span class="pre">T</span></code>. <code class="docutils literal notranslate"><span class="pre">c_n</span></code> is of shape <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">H]</span></code>
tensor containing the cell state for <code class="docutils literal notranslate"><span class="pre">t</span> <span class="pre">=</span> <span class="pre">T</span></code>.</p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="opacus.layers.dp_lstm.DPLSTMLayer.set_max_batch_length">
<span class="sig-name descname"><span class="pre">set_max_batch_length</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_batch_length</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/opacus/layers/dp_lstm.html#DPLSTMLayer.set_max_batch_length"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#opacus.layers.dp_lstm.DPLSTMLayer.set_max_batch_length" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets max batch length. Useful for PackedSequences</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.9)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></a></p>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="py class">
<dt class="sig sig-object py" id="opacus.layers.dp_lstm.LSTMLinear">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">opacus.layers.dp_lstm.</span></span><span class="sig-name descname"><span class="pre">LSTMLinear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/opacus/layers/dp_lstm.html#LSTMLinear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#opacus.layers.dp_lstm.LSTMLinear" title="Permalink to this definition">¶</a></dt>
<dd><p>This function is the same as a nn.Linear layer, except that in the backward pass
the grad_samples get accumulated (instead of being concatenated as in the standard
nn.Linear)</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
</dd></dl>
</section>
</div>
</div>
</div>
<div aria-label="main navigation" class="sphinxsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Opacus</a></h1>
<h3>Navigation</h3>
<p class="caption"><span class="caption-text">API Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="privacy_engine.html">Privacy Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="privacy_analysis.html">Privacy Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="per_sample_gradient_clip.html">Gradient Clipping</a></li>
<li class="toctree-l1"><a class="reference internal" href="per_sample_gradients.html">Per Sample Gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="dp_model_inspector.html">Model Validation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="layers.html">DP Layers</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="dp_multihead_attention.html">DPMultiheadAttention</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">DPLSTM</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">Utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="scripts.html">Scripts</a></li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
<li><a href="index.html">Documentation overview</a><ul>
<li><a href="layers.html">DP Layers</a><ul>
<li>Previous: <a href="dp_multihead_attention.html" title="previous chapter">DPMultiheadAttention</a></li>
<li>Next: <a href="utils.html" title="next chapter">Utils</a></li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="searchbox" role="search" style="display: none">
<h3 id="searchlabel">Quick search</h3>
<div class="searchformwrapper">
<form action="search.html" class="search" method="get">
<input aria-labelledby="searchlabel" name="q" type="text"/>
<input type="submit" value="Go"/>
</form>
</div>
</div>
<script>$('#searchbox').show(0);</script>
</div>
</div>
<div class="clearer"></div>
</div></div></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/" class="nav-home"><img src="/img/opacus_favicon.svg" alt="Opacus" width="66" height="58"/></a><div class="footerSection"><h5>Docs</h5><a href="/docs/introduction">Introduction</a><a href="/docs/faq">FAQ</a><a href="/tutorials/">Tutorials</a><a href="/api/">API Reference</a></div><div class="footerSection"><h5>Social</h5><div class="social"><a class="github-button" href="https://github.com/pytorch/opacus" data-count-href="https://github.com/pytorch/opacus/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star Opacus on GitHub">opacus</a></div></div><div class="footerSection"><h5>Legal</h5><a href="https://opensource.facebook.com/legal/privacy/" target="_blank" rel="noreferrer noopener">Privacy</a><a href="https://opensource.facebook.com/legal/terms/" target="_blank" rel="noreferrer noopener">Terms</a></div></section><a href="https://opensource.facebook.com/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/img/oss_logo.png" alt="Facebook Open Source" width="170" height="45"/></a><section class="copyright"> Copyright © 2021 Facebook Inc.</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
                document.addEventListener('keyup', function(e) {
                  if (e.target !== document.body) {
                    return;
                  }
                  // keyCode for '/' (slash)
                  if (e.keyCode === 191) {
                    const search = document.getElementById('search_input_react');
                    search && search.focus();
                  }
                });
              </script><script>
              var search = docsearch({
                
                apiKey: '207c27d819f967749142d8611de7cb19',
                indexName: 'opacus',
                inputSelector: '#search_input_react'
              });
            </script></body></html>